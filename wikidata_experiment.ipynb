{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from tqdm import tqdm\n",
    "from src.intension import Intension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [ \n",
    "    { \"model_name\": \"google/gemma-7b-it\", \"batch_size\": 50 },\n",
    "    { \"model_name\": \"gpt-3.5-turbo\", \"batch_size\": 50 },\n",
    "    { \"model_name\": \"gpt-4-0125-preview\", \"batch_size\": 50 },\n",
    "    { \"model_name\": \"mistralai/Mistral-7B-Instruct-v0.2\", \"batch_size\": 50 },\n",
    "    { \"model_name\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \"batch_size\": 50 },\n",
    "    { \"model_name\": \"claude-3-opus-20240229\", \"batch_size\": 1 },\n",
    "]\n",
    "\n",
    "DATA = json.load(open('experiments/wikidata_statements.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS:\n",
    "    filename = f'experiments/{model[\"model_name\"].split(\"/\")[-1]}-wikidata.json'\n",
    "    if os.path.isfile(filename):\n",
    "        print(f'Skipping {model[\"model_name\"]}')\n",
    "    else:\n",
    "        results = []\n",
    "        queries = [\n",
    "            {\n",
    "                \"predicate\": datum[\"predicate\"][\"label\"],\n",
    "                \"arguments\": \", \".join([ arg[\"label\"] for arg in datum[\"arguments\"] ]),\n",
    "                \"world\": datum[\"predicate\"][\"definition\"] + \" \".join([ arg[\"description\"] for arg in datum[\"arguments\"] ]),\n",
    "                \"actual\": datum[\"in_extension\"]\n",
    "            }\n",
    "            for datum in DATA\n",
    "        ]\n",
    "        batches = [ queries[i:i+model[\"batch_size\"]] for i in range(0, len(queries), model[\"batch_size\"]) ] \n",
    "        intension = Intension(model_name=model[\"model_name\"])\n",
    "        for batch in tqdm(batches, desc=f'{model[\"model_name\"]:30}', total=len(batches)):\n",
    "            response = intension.chain.batch(batch)\n",
    "            for i, result in enumerate(response):\n",
    "                result[\"rationale\"] = result[\"text\"][\"rationale\"]\n",
    "                result[\"predicted\"] = result[\"text\"][\"answer\"]\n",
    "                result.pop(\"text\")\n",
    "            results.extend(response)\n",
    "        json.dump(results, open(filename, \"w+\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conceptual-engineering-using-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
